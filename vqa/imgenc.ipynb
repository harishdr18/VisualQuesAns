{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global visual feature (F_v) shape: torch.Size([4, 1024])\n",
      "Concatenated visual feature shape: torch.Size([4, 11, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self, visual_dim, hidden_dim, num_layers):\n",
    "        super(VisualEncoder, self).__init__()\n",
    "        # Define the learnable visual token with the specified dimension\n",
    "        self.visual_token = nn.Parameter(torch.randn(1, 1, visual_dim))\n",
    "        \n",
    "        # Transformer-based visual encoding blocks\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [nn.TransformerEncoderLayer(d_model=visual_dim, nhead=8, dim_feedforward=hidden_dim)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, V):\n",
    "        # Batch size and visual feature dimensions\n",
    "        batch_size = V.size(0)\n",
    "        \n",
    "        # Concatenate the learnable visual token with the visual feature sequence\n",
    "        f_img_v = self.visual_token.expand(batch_size, -1, -1)\n",
    "        V_concat = torch.cat((f_img_v, V), dim=1)\n",
    "        \n",
    "        # Pass through stacked transformer encoder blocks\n",
    "        for layer in self.transformer_blocks:\n",
    "            V_concat = layer(V_concat)\n",
    "        \n",
    "        # Extract the global visual feature\n",
    "        F_v = V_concat[:, 0, :]  # Take the output corresponding to the visual token\n",
    "        \n",
    "        return F_v, V_concat  # Return both global and contextual features\n",
    "\n",
    "# Example usage\n",
    "visual_dim = 1024  # Dimensionality of visual features\n",
    "hidden_dim = 2048  # Hidden dimension for FFN\n",
    "num_layers = 5     # Number of stacked transformer blocks (NV)\n",
    "\n",
    "# Initialize the encoder\n",
    "visual_encoder = VisualEncoder(visual_dim=visual_dim, hidden_dim=hidden_dim, num_layers=num_layers)\n",
    "\n",
    "# Dummy visual features with batch size of 4 and 10 objects per image (just for example)\n",
    "V = torch.randn(4, 10, visual_dim)\n",
    "F_v, V_concat = visual_encoder(V)\n",
    "\n",
    "print(\"Global visual feature (F_v) shape:\", F_v.shape)\n",
    "print(\"Concatenated visual feature shape:\", V_concat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torch.utils.data import DataLoader\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Encoder Definition\n",
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self, visual_dim, hidden_dim, num_layers):\n",
    "        super(VisualEncoder, self).__init__()\n",
    "        self.visual_token = nn.Parameter(torch.randn(1, 1, visual_dim))\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [nn.TransformerEncoderLayer(d_model=visual_dim, nhead=8, dim_feedforward=hidden_dim)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, V):\n",
    "        batch_size = V.size(0)\n",
    "        f_img_v = self.visual_token.expand(batch_size, -1, -1)\n",
    "        V_concat = torch.cat((f_img_v, V), dim=1)\n",
    "        for layer in self.transformer_blocks:\n",
    "            V_concat = layer(V_concat)\n",
    "        F_v = V_concat[:, 0, :]  # Global feature\n",
    "        return F_v, V_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "visual_dim = 1024\n",
    "hidden_dim = 2048\n",
    "num_layers = 5\n",
    "batch_size = 4  \n",
    "image_size = 224\n",
    "K_v = 36  # Number of object features per image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Encoder\n",
    "visual_encoder = VisualEncoder(visual_dim=visual_dim, hidden_dim=hidden_dim, num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Faster R-CNN Model for Feature Extraction\n",
    "faster_rcnn = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "faster_rcnn.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=20.73s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Load COCO Dataset\n",
    "data_dir = 'D:/Project_phase_1/new_ds/train2014/train2014'  \n",
    "ann_file = 'D:/Project_phase_1/new_ds/annotations_trainval2014/annotations/instances_train2014.json'  \n",
    "coco = COCO(ann_file)\n",
    "img_ids = list(coco.imgs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, coco, img_dir, transform=None):\n",
    "        self.coco = coco\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.img_ids = list(coco.imgs.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "        image = Image.open(os.path.join(self.img_dir, path)).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return img_id, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "coco_dataset = COCODataset(coco, data_dir, transform=transform)\n",
    "data_loader = DataLoader(coco_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding and Saving Process\n",
    "output_dir = 'D:/Project_phase_1/image modality/im_encod'  \n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m output \u001b[38;5;241m=\u001b[39m faster_rcnn(batch)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, img_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(img_ids):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Collect features of detected objects\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     object_features \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Assuming output contains 'features' key\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     num_objects \u001b[38;5;241m=\u001b[39m object_features\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Select top K_v features (either pad or truncate to 36 features)\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'features'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for img_ids, batch in data_loader:\n",
    "        # Extract predictions (proposals, etc.) with Faster R-CNN\n",
    "        output = faster_rcnn(batch)\n",
    "        \n",
    "        for i, img_id in enumerate(img_ids):\n",
    "            # Extract the features for each region proposal\n",
    "            object_features = output[i]['boxes']  # Use box coordinates for ROI pooling if needed\n",
    "            proposals = len(object_features)\n",
    "\n",
    "            # Here we would typically run features through an ROI pooling layer to get 1024-dim vectors for each proposal\n",
    "            # Assuming we have `roi_pooled_features` as 1024-dim feature vectors for object proposals\n",
    "            \n",
    "            # Check the number of detected objects and adjust\n",
    "            if proposals >= K_v:\n",
    "                V = roi_pooled_features[:K_v]\n",
    "            else:\n",
    "                padding = torch.zeros(K_v - proposals, visual_dim)\n",
    "                V = torch.cat([roi_pooled_features, padding], dim=0)\n",
    "\n",
    "            V = V.unsqueeze(0)  # Add batch dimension to match encoder input shape\n",
    "            \n",
    "            # Pass features through the visual encoder\n",
    "            F_v, V_concat = visual_encoder(V)\n",
    "            \n",
    "            # Save encoded features in .pt format\n",
    "            save_path = os.path.join(output_dir, f\"{img_id}_features.pt\")\n",
    "            torch.save(F_v, save_path)\n",
    "            print(f\"Saved visual features for image {img_id} at {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=17.11s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 93\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, img_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(img_ids):\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Get bounding boxes and apply ROI Align to get object features\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m output[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Detected bounding boxes\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m     feature_maps \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Get the feature map associated with the predictions\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m boxes\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;66;03m# Convert boxes to numpy for roi_align\u001b[39;00m\n\u001b[0;32m     97\u001b[0m         boxes \u001b[38;5;241m=\u001b[39m boxes\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Convert to numpy for roi_align\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'features'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.ops import roi_align\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Visual Encoder Definition\n",
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self, visual_dim, hidden_dim, num_layers):\n",
    "        super(VisualEncoder, self).__init__()\n",
    "        self.visual_token = nn.Parameter(torch.randn(1, 1, visual_dim))\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [nn.TransformerEncoderLayer(d_model=visual_dim, nhead=8, dim_feedforward=hidden_dim)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, V):\n",
    "        batch_size = V.size(0)\n",
    "        f_img_v = self.visual_token.expand(batch_size, -1, -1)\n",
    "        V_concat = torch.cat((f_img_v, V), dim=1)\n",
    "        for layer in self.transformer_blocks:\n",
    "            V_concat = layer(V_concat)\n",
    "        F_v = V_concat[:, 0, :]  # Global feature\n",
    "        return F_v, V_concat\n",
    "\n",
    "# Parameters\n",
    "visual_dim = 1024\n",
    "hidden_dim = 2048\n",
    "num_layers = 5\n",
    "batch_size = 4  # Process images in batches of 4\n",
    "image_size = 800  # Use 800 for resizing to match Faster R-CNN input size\n",
    "K_v = 36  # Number of object features per image\n",
    "\n",
    "# Initialize Encoder\n",
    "visual_encoder = VisualEncoder(visual_dim=visual_dim, hidden_dim=hidden_dim, num_layers=num_layers)\n",
    "\n",
    "# Faster R-CNN Model for Feature Extraction\n",
    "faster_rcnn = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "faster_rcnn.eval()\n",
    "\n",
    "# Image Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load COCO Dataset\n",
    "data_dir = 'D:/Project_phase_1/new_ds/train2014/train2014'  # COCO dataset path\n",
    "ann_file = 'D:/Project_phase_1/new_ds/annotations_trainval2014/annotations/instances_train2014.json'  # Annotations path\n",
    "coco = COCO(ann_file)\n",
    "img_ids = list(coco.imgs.keys())\n",
    "\n",
    "class COCODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, coco, img_dir, transform=None):\n",
    "        self.coco = coco\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.img_ids = list(coco.imgs.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "        image = Image.open(os.path.join(self.img_dir, path)).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return img_id, image\n",
    "\n",
    "# Dataloader\n",
    "coco_dataset = COCODataset(coco, data_dir, transform=transform)\n",
    "data_loader = DataLoader(coco_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Directory to save encoded features\n",
    "output_dir = 'D:/Project_phase_1/image modality/im_encod'  # Output path for saved features\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Encoding and Saving Process\n",
    "with torch.no_grad():\n",
    "    for img_ids, batch in data_loader:\n",
    "        # Extract predictions and feature maps with Faster R-CNN\n",
    "        output = faster_rcnn(batch)\n",
    "        \n",
    "        for i, img_id in enumerate(img_ids):\n",
    "            # Get bounding boxes and apply ROI Align to get object features\n",
    "            boxes = output[i]['boxes']  # Detected bounding boxes\n",
    "            feature_maps = output[i]['features']  # Get the feature map associated with the predictions\n",
    "            \n",
    "            if boxes.shape[0] > 0:\n",
    "                # Convert boxes to numpy for roi_align\n",
    "                boxes = boxes.cpu().detach().numpy()  # Convert to numpy for roi_align\n",
    "                object_features = roi_align(feature_maps, boxes, output_size=(1, 1))\n",
    "                object_features = object_features.view(object_features.size(0), -1)  # Flatten to (num_objects, 1024)\n",
    "            else:\n",
    "                object_features = torch.zeros(0, visual_dim)  # Handle images with no detections\n",
    "\n",
    "            num_objects = object_features.size(0)\n",
    "\n",
    "            # Select top K_v features (either pad or truncate to 36 features)\n",
    "            if num_objects >= K_v:\n",
    "                V = object_features[:K_v]\n",
    "            else:\n",
    "                padding = torch.zeros(K_v - num_objects, visual_dim)\n",
    "                V = torch.cat([object_features, padding], dim=0)\n",
    "\n",
    "            V = V.unsqueeze(0)  # Add batch dimension to match encoder input shape\n",
    "\n",
    "            # Pass features through the visual encoder\n",
    "            F_v, V_concat = visual_encoder(V)\n",
    "\n",
    "            # Save encoded features in .pt format\n",
    "            save_path = os.path.join(output_dir, f\"{img_id}_features.pt\")\n",
    "            torch.save(F_v, save_path)\n",
    "            print(f\"Saved visual features for image {img_id} at {save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=17.03s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.datasets import CocoDetection\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# %%\n",
    "# Define preprocessing for the images\n",
    "image_size = 800  # Resize to the specified size\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# %%\n",
    "# Load pretrained Faster R-CNN model\n",
    "faster_rcnn = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "faster_rcnn.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# %%\n",
    "# Position-Aware Representation Module\n",
    "class PositionAwareRepresentation(nn.Module):\n",
    "    def __init__(self, feature_dim, bbox_dim, output_dim, hidden_dim=2048, num_layers=5):\n",
    "        super(PositionAwareRepresentation, self).__init__()\n",
    "        \n",
    "        # Fully connected layers with the specified hidden dimensions and layers\n",
    "        self.fc_f = nn.Linear(feature_dim, hidden_dim)\n",
    "        self.fc_b = nn.Linear(bbox_dim, hidden_dim)\n",
    "        \n",
    "        self.hidden_layers = nn.Sequential(\n",
    "            *[nn.Linear(hidden_dim, hidden_dim) for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        self.layer_norm_f = nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm_b = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, features, bboxes):\n",
    "        # Normalize features and bounding boxes\n",
    "        normalized_features = self.layer_norm_f(self.fc_f(features))\n",
    "        normalized_bboxes = self.layer_norm_b(self.fc_b(bboxes))\n",
    "        \n",
    "        # Sum and average\n",
    "        vj = (normalized_features + normalized_bboxes) / 2\n",
    "        \n",
    "        # Pass through hidden layers and final output layer\n",
    "        V = self.output_layer(self.hidden_layers(vj).sum(dim=0))  # Sum over KV (36 objects)\n",
    "        return V\n",
    "\n",
    "# %%\n",
    "# Function to extract features and bounding boxes from Faster R-CNN\n",
    "def extract_faster_rcnn_features(image_tensor, kv=36):\n",
    "    with torch.no_grad():\n",
    "        predictions = faster_rcnn(image_tensor)\n",
    "\n",
    "    objects = predictions[0]\n",
    "    bboxes = objects['boxes']  # Shape: [N, 4]\n",
    "    features = objects['scores']  # Shape: [N] confidence scores\n",
    "\n",
    "    if len(features) > kv:\n",
    "        features = features[:kv]\n",
    "        bboxes = bboxes[:kv]\n",
    "    elif len(features) < kv:\n",
    "        padding_size = kv - len(features)\n",
    "        features = torch.cat((features, torch.zeros(padding_size)))\n",
    "        bboxes = torch.cat((bboxes, torch.zeros((padding_size, 4))))\n",
    "\n",
    "    return features, bboxes\n",
    "\n",
    "# %%\n",
    "def process_dataset(coco_dataset, batch_size=4, kv=36, output_file='output_embeddings.pt'):\n",
    "    results = []\n",
    "    num_images = len(coco_dataset)\n",
    "\n",
    "    # Set model dimensions\n",
    "    dv = 1024  # visual_dim output dimension\n",
    "    model = PositionAwareRepresentation(feature_dim=kv, bbox_dim=4, output_dim=dv)\n",
    "\n",
    "    for i in range(len(coco_dataset.ids)):\n",
    "        image, _ = coco_dataset[i]\n",
    "        image_tensor = preprocess(image).unsqueeze(0)\n",
    "        \n",
    "        features, bboxes = extract_faster_rcnn_features(image_tensor, kv)\n",
    "\n",
    "        V = model(features.unsqueeze(0), bboxes.unsqueeze(0))\n",
    "\n",
    "        annotations = coco_dataset.coco.imgToAnns.get(coco_dataset.ids[i], [])\n",
    "        if annotations:\n",
    "            image_id = annotations[0]['image_id']\n",
    "            results.append({'image_id': image_id, 'embedding': V})\n",
    "\n",
    "        if (i + 1) % batch_size == 0 or (i + 1) == num_images:\n",
    "            torch.save(results, output_file)\n",
    "            results = []\n",
    "\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f'Processed {i + 1}/{num_images} images')\n",
    "\n",
    "    print(f'Results saved to {output_file}')\n",
    "\n",
    "# %%\n",
    "# MS COCO dataset paths\n",
    "coco_root = 'D:/Project_phase_1/new_ds/'\n",
    "train_annotations = os.path.join(coco_root, 'annotations_trainval2014/annotations/instances_train2014.json')\n",
    "train_images_dir = os.path.join(coco_root, 'train2014/train2014/')\n",
    "\n",
    "# %%\n",
    "# Load the MS COCO dataset for training\n",
    "coco_dataset = CocoDetection(root=train_images_dir, annFile=train_annotations)\n",
    "\n",
    "# %%\n",
    "# Process the dataset and save results\n",
    "process_dataset(coco_dataset, batch_size=4, kv=36, output_file='output_img.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
