{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "import torch.nn.functional as Fnn\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageModalityEncoder(nn.Module):\n",
    "    def __init__(self, num_objects=100, vector_dim=256):\n",
    "        super(ImageModalityEncoder, self).__init__()\n",
    "        self.num_objects = num_objects\n",
    "        self.vector_dim = vector_dim\n",
    "\n",
    "        # Load pretrained Faster R-CNN model\n",
    "        self.faster_rcnn = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "        self.faster_rcnn.eval()  # Set model to eval mode\n",
    "\n",
    "        # Define the learnable parameters for position-aware encoding\n",
    "        self.Wf = nn.Linear(1024, vector_dim)\n",
    "        self.Wb = nn.Linear(4, vector_dim)\n",
    "        self.bf = nn.Parameter(torch.zeros(vector_dim))\n",
    "        self.bb = nn.Parameter(torch.zeros(vector_dim))\n",
    "\n",
    "        # Layer normalization\n",
    "        self.norm_f = nn.LayerNorm(vector_dim)\n",
    "        self.norm_b = nn.LayerNorm(vector_dim)\n",
    "\n",
    "    def forward(self, image):\n",
    "        # Register hook to capture features from the backbone\n",
    "        feature_maps = []\n",
    "        \n",
    "        def hook_fn(module, input, output):\n",
    "            feature_maps.append(output)\n",
    "        \n",
    "        hook = self.faster_rcnn.backbone.register_forward_hook(hook_fn)\n",
    "        \n",
    "        # Apply Faster R-CNN to extract object detections\n",
    "        with torch.no_grad():\n",
    "            predictions = self.faster_rcnn([image])\n",
    "        \n",
    "        hook.remove()  # Remove the hook after the forward pass\n",
    "        \n",
    "        # The feature maps from the backbone are now stored in `feature_maps`\n",
    "        feature_map = feature_maps[0]  # [batch_size, channels, height, width]\n",
    "        \n",
    "        # Get the top `num_objects` detected objects (bounding boxes and features)\n",
    "        boxes = predictions[0]['boxes'][:self.num_objects]  # [num_objects, 4]\n",
    "\n",
    "        # Normalize bounding boxes to the feature map dimensions\n",
    "        image_height, image_width = image.shape[-2:]\n",
    "        feature_map_height, feature_map_width = feature_map.shape[-2:]\n",
    "\n",
    "        # Normalize bounding box coordinates to [0, 1] with respect to image size\n",
    "        boxes[:, [0, 2]] /= image_width\n",
    "        boxes[:, [1, 3]] /= image_height\n",
    "\n",
    "        # Scale boxes to the feature map size\n",
    "        boxes[:, [0, 2]] *= feature_map_width\n",
    "        boxes[:, [1, 3]] *= feature_map_height\n",
    "\n",
    "        # Add batch index (since RoIAlign expects [batch_idx, x1, y1, x2, y2])\n",
    "        box_indices = torch.zeros(boxes.size(0), dtype=torch.int, device=boxes.device)\n",
    "        boxes_with_indices = torch.cat([box_indices[:, None], boxes], dim=1)\n",
    "\n",
    "        # Use RoIAlign to pool features for each bounding box\n",
    "        roi_align = torchvision.ops.RoIAlign(output_size=(7, 7), spatial_scale=1.0, sampling_ratio=-1)\n",
    "        pooled_features = roi_align(feature_map, [boxes_with_indices])  # [num_objects, channels, 7, 7]\n",
    "        \n",
    "        # Flatten pooled features and encode them\n",
    "        pooled_features = pooled_features.view(pooled_features.size(0), -1)  # [num_objects, channels * 7 * 7]\n",
    "        \n",
    "        # Normalize and encode features and bounding box coordinates\n",
    "        fj = self.norm_f(self.Wf(pooled_features)) + self.bf  # [num_objects, vector_dim]\n",
    "        bj = self.norm_b(self.Wb(boxes)) + self.bb  # [num_objects, vector_dim]\n",
    "\n",
    "        # Compute the final position-aware object representations\n",
    "        vj = (fj + bj) / 2  # [num_objects, vector_dim]\n",
    "\n",
    "        # Aggregate all object representations to get the final image representation\n",
    "        V = torch.mean(vj, dim=0)  # [vector_dim]\n",
    "        \n",
    "        return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "def preprocess_image(image_path):\n",
    "    from PIL import Image\n",
    "    image = Image.open(image_path)\n",
    "    image = F.to_tensor(image)  # Convert image to tensor\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "image_encoder = ImageModalityEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess an example image\n",
    "image_tensor = preprocess_image(\"D:/Project_phase_1/train2014/COCO_train2014_000000000009.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a batch dimension\n",
    "image_tensor = image_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions Output:  [{'boxes': tensor([[3.1598e+02, 1.5025e+00, 6.1762e+02, 2.3067e+02],\n",
      "        [2.5639e+02, 2.3405e+02, 5.7711e+02, 4.6684e+02],\n",
      "        [0.0000e+00, 1.2453e+01, 4.3430e+02, 3.8766e+02],\n",
      "        [3.9088e+02, 7.3975e+01, 4.6952e+02, 1.4071e+02],\n",
      "        [4.6476e+02, 4.0634e+01, 5.2319e+02, 8.9265e+01],\n",
      "        [3.3420e+01, 1.9109e+02, 6.1583e+02, 4.8000e+02],\n",
      "        [3.6256e+02, 2.6677e+00, 4.5770e+02, 6.7999e+01],\n",
      "        [3.2560e+02, 4.9800e+00, 4.9315e+02, 1.3748e+02],\n",
      "        [4.6027e+02, 5.5890e+01, 5.7009e+02, 1.4032e+02],\n",
      "        [3.7236e+02, 3.6787e+01, 4.5330e+02, 8.3875e+01],\n",
      "        [0.0000e+00, 1.1023e+02, 2.3114e+02, 3.0810e+02],\n",
      "        [5.1149e+02, 2.0075e+02, 6.4000e+02, 3.2278e+02],\n",
      "        [0.0000e+00, 1.1126e+02, 2.2888e+02, 3.0539e+02],\n",
      "        [1.3820e+02, 5.6518e+00, 5.4538e+02, 2.6526e+02],\n",
      "        [5.0799e+02, 2.0302e+02, 6.4000e+02, 3.6213e+02],\n",
      "        [9.1573e+00, 3.1162e+02, 3.5753e+02, 4.8000e+02],\n",
      "        [3.0671e+02, 3.7328e+00, 4.5103e+02, 1.9408e+02],\n",
      "        [0.0000e+00, 8.6516e+01, 2.4717e+02, 3.5108e+02],\n",
      "        [5.1128e+02, 2.0042e+02, 6.4000e+02, 3.3259e+02],\n",
      "        [3.2931e+02, 4.1048e+02, 4.2290e+02, 4.7863e+02],\n",
      "        [5.9055e+00, 3.3929e+01, 6.2840e+02, 4.6179e+02],\n",
      "        [2.0156e+02, 4.4168e+01, 3.7611e+02, 2.2799e+02],\n",
      "        [3.6740e+02, 3.2300e+01, 4.6638e+02, 1.3683e+02],\n",
      "        [3.5212e+00, 7.3920e-02, 3.5284e+02, 1.3029e+02],\n",
      "        [3.4651e+02, 1.8238e+02, 6.2278e+02, 4.3535e+02],\n",
      "        [1.7123e+01, 4.5138e+00, 3.6459e+02, 1.4167e+02],\n",
      "        [4.6403e+02, 5.2666e+01, 5.5181e+02, 1.2637e+02],\n",
      "        [2.5321e+02, 1.2151e+02, 6.0217e+02, 3.9949e+02],\n",
      "        [2.0429e+02, 4.7047e+01, 3.7844e+02, 2.2815e+02],\n",
      "        [5.1436e+02, 9.9891e+01, 6.3841e+02, 4.1291e+02],\n",
      "        [4.6250e+02, 5.2184e+01, 5.7537e+02, 1.4195e+02],\n",
      "        [3.6550e+02, 2.2010e+01, 4.9182e+02, 8.7542e+01],\n",
      "        [5.6175e+00, 1.0390e+02, 2.2862e+02, 3.2981e+02],\n",
      "        [1.2263e+02, 1.1238e+02, 1.5080e+02, 1.4666e+02],\n",
      "        [4.4462e+02, 3.8191e+01, 4.7822e+02, 8.5985e+01],\n",
      "        [3.7714e+02, 2.0151e+01, 5.8387e+02, 1.4376e+02],\n",
      "        [0.0000e+00, 1.5751e+02, 3.1743e+02, 4.5589e+02],\n",
      "        [0.0000e+00, 1.1161e+02, 2.2976e+02, 3.0126e+02],\n",
      "        [4.0506e+01, 3.3416e+02, 2.8540e+02, 4.7168e+02],\n",
      "        [3.0248e+02, 3.6073e+02, 4.2446e+02, 4.7557e+02],\n",
      "        [7.4300e+00, 5.4393e+01, 1.7087e+02, 1.3844e+02],\n",
      "        [1.7303e+02, 3.8521e+01, 2.6955e+02, 1.6807e+02],\n",
      "        [9.6362e+01, 3.4183e+02, 3.3111e+02, 4.7253e+02],\n",
      "        [7.1935e+00, 9.0522e+01, 2.3141e+02, 2.9966e+02],\n",
      "        [3.9817e+02, 6.9761e+01, 6.2583e+02, 2.9395e+02],\n",
      "        [3.7465e+02, 2.1774e+01, 5.8670e+02, 1.4326e+02],\n",
      "        [6.6175e+01, 3.4150e+02, 3.3633e+02, 4.7554e+02],\n",
      "        [3.1506e+00, 5.7826e+01, 1.6911e+02, 1.3686e+02],\n",
      "        [2.5370e+02, 2.6188e+01, 4.2636e+02, 1.8118e+02],\n",
      "        [9.2777e+01, 3.4480e+02, 3.2429e+02, 4.7058e+02],\n",
      "        [4.1120e+00, 1.8220e+02, 3.1935e+02, 4.6766e+02],\n",
      "        [5.4832e+02, 2.5496e+00, 6.3816e+02, 1.6393e+02],\n",
      "        [4.5685e+02, 2.0976e+02, 6.4000e+02, 4.6145e+02],\n",
      "        [8.8314e+01, 3.0585e+02, 5.2432e+02, 4.7819e+02],\n",
      "        [2.5045e+02, 2.7922e+02, 4.4233e+02, 4.7361e+02],\n",
      "        [2.0070e+02, 4.0353e+01, 3.7743e+02, 2.2572e+02],\n",
      "        [3.9925e+02, 2.2451e+02, 6.0586e+02, 4.5410e+02],\n",
      "        [4.2766e+02, 3.6814e+01, 5.2081e+02, 9.5347e+01],\n",
      "        [0.0000e+00, 1.1025e+02, 2.3478e+02, 3.0218e+02],\n",
      "        [3.0268e+00, 0.0000e+00, 5.1185e+02, 2.2891e+02],\n",
      "        [1.7134e+02, 3.7929e+01, 2.7070e+02, 1.6997e+02],\n",
      "        [5.1186e+02, 1.9967e+02, 6.4000e+02, 3.3695e+02],\n",
      "        [2.2383e+02, 9.1284e+01, 6.2483e+02, 4.0149e+02],\n",
      "        [1.7064e+02, 3.7576e+01, 2.7613e+02, 1.8158e+02]]), 'labels': tensor([51, 56, 51, 55, 55, 51, 55, 55, 53, 55, 61, 60, 54, 51, 51, 51, 51, 51,\n",
      "        61, 56, 67, 61, 55, 67, 51, 51, 55, 51, 54, 51, 52, 55, 52, 57, 55, 55,\n",
      "        51, 60, 52, 56, 51, 52, 56, 53, 51, 53, 60, 67, 55, 57, 67, 51, 67, 56,\n",
      "        56, 60, 56, 55, 58, 67, 60, 54, 67, 55]), 'scores': tensor([0.9892, 0.9892, 0.8844, 0.8365, 0.7939, 0.7679, 0.7144, 0.6939, 0.6742,\n",
      "        0.6723, 0.6571, 0.6240, 0.5781, 0.5667, 0.4865, 0.4754, 0.4736, 0.3729,\n",
      "        0.3588, 0.3579, 0.3394, 0.3362, 0.3289, 0.2868, 0.2820, 0.2457, 0.2207,\n",
      "        0.1992, 0.1970, 0.1948, 0.1887, 0.1822, 0.1617, 0.1545, 0.1483, 0.1179,\n",
      "        0.1172, 0.1029, 0.1002, 0.0989, 0.0977, 0.0943, 0.0917, 0.0849, 0.0839,\n",
      "        0.0823, 0.0804, 0.0796, 0.0777, 0.0753, 0.0717, 0.0690, 0.0683, 0.0665,\n",
      "        0.0651, 0.0648, 0.0646, 0.0640, 0.0608, 0.0596, 0.0588, 0.0580, 0.0576,\n",
      "        0.0530])}]\n",
      "torch.Size([14, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from PIL import Image\n",
    "\n",
    "# Check if a GPU is available and use it if possible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the Image Encoder class\n",
    "class ImageEncoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.faster_rcnn = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "        self.faster_rcnn.eval()  # Set to evaluation mode\n",
    "\n",
    "    def forward(self, image):\n",
    "        # Ensure image is on the same device as the model\n",
    "        image = image.to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = self.faster_rcnn([image])  # Pass image as a list\n",
    "        print(\"Predictions Output: \", predictions)\n",
    "        \n",
    "        # Example processing for the output\n",
    "        boxes = predictions[0]['boxes']\n",
    "        labels = predictions[0]['labels']\n",
    "        scores = predictions[0]['scores']\n",
    "\n",
    "        # Use only boxes with a score > threshold\n",
    "        threshold = 0.5\n",
    "        valid_indices = scores > threshold\n",
    "        boxes = boxes[valid_indices]\n",
    "        \n",
    "        # Further processing can go here (e.g., RoIAlign, etc.)\n",
    "\n",
    "        return boxes  # Return the processed boxes or other features\n",
    "\n",
    "# Create an instance of the ImageEncoder\n",
    "image_encoder = ImageEncoder().to(device)\n",
    "\n",
    "# Load an image from the MS COCO dataset\n",
    "image_path = 'D:/Project_phase_1/train2014/COCO_train2014_000000000009.jpg'  # Update this path to your image file\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Resize((480, 640)),  # Resize if necessary\n",
    "])\n",
    "\n",
    "# Preprocess the image\n",
    "image_tensor = transform(image).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "# Get the final image representation\n",
    "image_representation = image_encoder(image_tensor.squeeze(0))  # Pass image tensor directly\n",
    "print(image_representation.shape)  # Output the final vector dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
